[
  {
    "objectID": "football.html",
    "href": "football.html",
    "title": "Exploring Regularization Techniques in Neural Networks for French Football Corporation",
    "section": "",
    "text": "The application of neural networks in sports analytics has gained immense traction, revolutionizing strategies and decision-making processes. The French Football Corporation, seeking to optimize their team’s gameplay, has enlisted the expertise of an AI specialist. The task at hand involves recommending optimal positions for France’s goalkeepers to kick the ball, strategically positioning it for the team’s players to execute successful headers.\nThis project aims to delve into the intricate realm of neural networks, exploring the effects of different regularization techniques – specifically, L2 regularization and Dropout. The analysis involves building a three-layer neural network from scratch using fundamental Python libraries like NumPy and SciPy, encapsulated within a Jupyter notebook environment.\nThrough this investigation, the project endeavors to understand and demonstrate the impact of regularization on the network’s performance, focusing on mitigating overfitting and enhancing the model’s generalizability. Leveraging various datasets and visualization tools, the study aims to elucidate the nuances of these techniques and their implications in optimizing strategic decision-making in football gameplay.\nThe project not only emphasizes the technical implementation of regularization but also underscores its practical significance in sports analytics, particularly in the realm of football strategies for the French team. This exploration marks a significant step towards employing cutting-edge AI methodologies to enhance performance in sports scenarios, reflecting the intersection of technology and athletics."
  },
  {
    "objectID": "football.html#introduction",
    "href": "football.html#introduction",
    "title": "Exploring Regularization Techniques in Neural Networks for French Football Corporation",
    "section": "",
    "text": "The application of neural networks in sports analytics has gained immense traction, revolutionizing strategies and decision-making processes. The French Football Corporation, seeking to optimize their team’s gameplay, has enlisted the expertise of an AI specialist. The task at hand involves recommending optimal positions for France’s goalkeepers to kick the ball, strategically positioning it for the team’s players to execute successful headers.\nThis project aims to delve into the intricate realm of neural networks, exploring the effects of different regularization techniques – specifically, L2 regularization and Dropout. The analysis involves building a three-layer neural network from scratch using fundamental Python libraries like NumPy and SciPy, encapsulated within a Jupyter notebook environment.\nThrough this investigation, the project endeavors to understand and demonstrate the impact of regularization on the network’s performance, focusing on mitigating overfitting and enhancing the model’s generalizability. Leveraging various datasets and visualization tools, the study aims to elucidate the nuances of these techniques and their implications in optimizing strategic decision-making in football gameplay.\nThe project not only emphasizes the technical implementation of regularization but also underscores its practical significance in sports analytics, particularly in the realm of football strategies for the French team. This exploration marks a significant step towards employing cutting-edge AI methodologies to enhance performance in sports scenarios, reflecting the intersection of technology and athletics."
  },
  {
    "objectID": "football.html#data-loading-and-exploration",
    "href": "football.html#data-loading-and-exploration",
    "title": "Exploring Regularization Techniques in Neural Networks for French Football Corporation",
    "section": "Data Loading and Exploration",
    "text": "Data Loading and Exploration\n\nLibrary Import and Configuration\nThis section focuses on importing essential libraries necessary for the project. It includes the following libraries:\n\nNumPy: Used for numerical computations and array operations.\nMatplotlib: Utilized for data visualization and plotting graphs.\nScikit-learn (sklearn): Provides machine learning algorithms and tools.\nSciPy (scipy.io): Used for scientific computing and handling input/output operations.\n\nAdditionally, the code snippet configures the default parameters for Matplotlib plots, setting the figure size, interpolation method, and color map for visualizations within the notebook environment.\nThe %matplotlib inline command ensures that Matplotlib plots are displayed inline within the Jupyter Notebook, allowing for immediate visualization of graphs and figures.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nimport sklearn.datasets\nimport scipy.io\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'"
  },
  {
    "objectID": "football.html#dataset-loading-and-visualization",
    "href": "football.html#dataset-loading-and-visualization",
    "title": "Exploring Regularization Techniques in Neural Networks for French Football Corporation",
    "section": "Dataset Loading and Visualization",
    "text": "Dataset Loading and Visualization\nDescription: This section handles the loading and preparation of the dataset required for the French Football Corporation’s analysis. It involves importing the dataset using scipy.io.loadmat, followed by partitioning it into training and testing subsets (train_X, train_Y, test_X, test_Y). Additionally, a scatter plot visualization using plt.scatter displays the distribution of the training data, aiding in understanding the dataset’s structure and characteristics.\n\ndata = scipy.io.loadmat('datasets/data.mat')\ntrain_X = data['X'].T\ntrain_Y = data['y'].T\ntest_X = data['Xval'].T\ntest_Y = data['yval'].T\n\nplt.scatter(train_X[0, :], train_X[1, :], c=train_Y, s=40, cmap=plt.cm.Spectral);"
  },
  {
    "objectID": "football.html#neural-network-implementation",
    "href": "football.html#neural-network-implementation",
    "title": "Exploring Regularization Techniques in Neural Networks for French Football Corporation",
    "section": "Neural Network Implementation",
    "text": "Neural Network Implementation\n\nNeural Network Implementation for Binary Classification\nThis section of the code defines fundamental functions required to build and train a neural network for binary classification. It begins with essential activation functions like sigmoid and ReLU, necessary for the network’s forward propagation.\n\nSigmoid and ReLU Functions:\n\nsigmoid(x): Computes the sigmoid function for scalar or numpy arrays.\nrelu(x): Calculates the rectified linear unit (ReLU) for scalar or numpy arrays.\n\n\n\ndef sigmoid(x):\n    \"\"\"\n    Compute the sigmoid of x\n\n    Arguments:\n    x -- A scalar or numpy array of any size.\n\n    Return:\n    s -- sigmoid(x)\n    \"\"\"\n    s = 1/(1+np.exp(-x))\n    return s\n\ndef relu(x):\n    \"\"\"\n    Compute the relu of x\n\n    Arguments:\n    x -- A scalar or numpy array of any size.\n\n    Return:\n    s -- relu(x)\n    \"\"\"\n    s = np.maximum(0,x)\n    \n    return s\n\n\nParameter Initialization:\n\ninitialize_parameters(layer_dims): Initializes the weights and biases for the neural network layers using a Python dictionary, given the dimensions of each layer.\n\n\n\ndef initialize_parameters(layer_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the dimensions of each layer in our network\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    W1 -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n                    b1 -- bias vector of shape (layer_dims[l], 1)\n                    Wl -- weight matrix of shape (layer_dims[l-1], layer_dims[l])\n                    bl -- bias vector of shape (1, layer_dims[l])\n                    \n    Tips:\n    - For example: the layer_dims for the \"Planar Data classification model\" would have been [2,2,1]. \n    This means W1's shape was (2,2), b1 was (1,2), W2 was (2,1) and b2 was (1,1). Now you have to generalize it!\n    - In the for loop, use parameters['W' + str(l)] to access Wl, where l is the iterative integer.\n    \"\"\"\n    \n    np.random.seed(3)\n    parameters = {}\n    L = len(layer_dims) # number of layers in the network\n\n    for l in range(1, L):\n        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])\n        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n        \n        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n\n        \n    return parameters\n\n\nForward Propagation:\n\nforward_propagation(X, parameters): Implements the forward propagation steps through the neural network layers, culminating in the output layer’s sigmoid activation.\n\n\n\ndef forward_propagation(X, parameters):\n    \"\"\"\n    Implements the forward propagation (and computes the loss) presented in Figure 2.\n    \n    Arguments:\n    X -- input dataset, of shape (input size, number of examples)\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n                    W1 -- weight matrix of shape ()\n                    b1 -- bias vector of shape ()\n                    W2 -- weight matrix of shape ()\n                    b2 -- bias vector of shape ()\n                    W3 -- weight matrix of shape ()\n                    b3 -- bias vector of shape ()\n    \n    Returns:\n    loss -- the loss function (vanilla logistic loss)\n    \"\"\"\n        \n    # retrieve parameters\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    W3 = parameters[\"W3\"]\n    b3 = parameters[\"b3\"]\n    \n    # LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID\n    Z1 = np.dot(W1, X) + b1\n    A1 = relu(Z1)\n    Z2 = np.dot(W2, A1) + b2\n    A2 = relu(Z2)\n    Z3 = np.dot(W3, A2) + b3\n    A3 = sigmoid(Z3)\n    \n    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n    \n    return A3, cache\n\n\nBackward Propagation:\n\nbackward_propagation(X, Y, cache): Implements the backward propagation, computing gradients with respect to each parameter and activation variable using the cached values from forward propagation.\n\n\n\ndef backward_propagation(X, Y, cache):\n    \"\"\"\n    Implement the backward propagation presented in figure 2.\n    \n    Arguments:\n    X -- input dataset, of shape (input size, number of examples)\n    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat)\n    cache -- cache output from forward_propagation()\n    \n    Returns:\n    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n    \"\"\"\n    m = X.shape[1]\n    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n    \n    dZ3 = A3 - Y\n    dW3 = 1./m * np.dot(dZ3, A2.T)\n    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n    \n    dA2 = np.dot(W3.T, dZ3)\n    dZ2 = np.multiply(dA2, np.int64(A2 &gt; 0))\n    dW2 = 1./m * np.dot(dZ2, A1.T)\n    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n    \n    dA1 = np.dot(W2.T, dZ2)\n    dZ1 = np.multiply(dA1, np.int64(A1 &gt; 0))\n    dW1 = 1./m * np.dot(dZ1, X.T)\n    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n    \n    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n    \n    return gradients\n\n\nParameter Updates using Gradient Descent:\n\nupdate_parameters(parameters, grads, learning_rate): Updates the parameters (weights and biases) using the calculated gradients and a specified learning rate.\n\n\n\ndef update_parameters(parameters, grads, learning_rate):\n    \"\"\"\n    Update parameters using gradient descent\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters:\n                    parameters['W' + str(i)] = Wi\n                    parameters['b' + str(i)] = bi\n    grads -- python dictionary containing your gradients for each parameters:\n                    grads['dW' + str(i)] = dWi\n                    grads['db' + str(i)] = dbi\n    learning_rate -- the learning rate, scalar.\n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n    \n    n = len(parameters) // 2 # number of layers in the neural networks\n\n    # Update rule for each parameter\n    for k in range(n):\n        parameters[\"W\" + str(k+1)] = parameters[\"W\" + str(k+1)] - learning_rate * grads[\"dW\" + str(k+1)]\n        parameters[\"b\" + str(k+1)] = parameters[\"b\" + str(k+1)] - learning_rate * grads[\"db\" + str(k+1)]\n        \n    return parameters\n\n\nPrediction and Evaluation:\n\npredict(X, y, parameters): Predicts outcomes based on the trained neural network and computes accuracy for the given dataset.\ncompute_cost(a3, Y): Computes the cost function for the binary classification model using the logistic loss.\n\n\n\ndef predict(X, y, parameters):\n    \"\"\"\n    This function is used to predict the results of a  n-layer neural network.\n    \n    Arguments:\n    X -- data set of examples you would like to label\n    parameters -- parameters of the trained model\n    \n    Returns:\n    p -- predictions for the given dataset X\n    \"\"\"\n    \n    m = X.shape[1]\n    p = np.zeros((1,m), dtype = int)\n    \n    # Forward propagation\n    a3, caches = forward_propagation(X, parameters)\n    \n    # convert probas to 0/1 predictions\n    for i in range(0, a3.shape[1]):\n        if a3[0,i] &gt; 0.5:\n            p[0,i] = 1\n        else:\n            p[0,i] = 0\n\n    print(\"Accuracy: \"  + str(np.mean((p[0,:] == y[0,:]))))\n    \n    return p\n\ndef compute_cost(a3, Y):\n    \"\"\"\n    Implement the cost function\n    \n    Arguments:\n    a3 -- post-activation, output of forward propagation\n    Y -- \"true\" labels vector, same shape as a3\n    \n    Returns:\n    cost - value of the cost function\n    \"\"\"\n    m = Y.shape[1]\n    \n    logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)\n    cost = 1./m * np.nansum(logprobs)\n    \n    return cost\n\nThese functions are essential building blocks necessary to construct, train, and evaluate a neural network for binary classification tasks. They perform forward and backward propagation, parameter initialization, updating, prediction, and cost computation, making them fundamental to constructing a neural network model.\n\n\nNeural Network Model with Regularization and Dropout Techniques\nThis section of the code presents a comprehensive neural network model incorporating various functionalities crucial for robust and optimized training:\n\nModel Construction:\n\nmodel(X, Y, learning_rate, num_iterations, print_cost, lambd, keep_prob): Implements a three-layer neural network with optional features for L2 regularization and dropout. It performs forward and backward propagation while updating parameters iteratively through gradient descent.\n\n\n\ndef model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):\n    \"\"\"\n    Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)\n    learning_rate -- learning rate of the optimization\n    num_iterations -- number of iterations of the optimization loop\n    print_cost -- If True, print the cost every 10000 iterations\n    lambd -- regularization hyperparameter, scalar\n    keep_prob - probability of keeping a neuron active during drop-out, scalar.\n    \n    Returns:\n    parameters -- parameters learned by the model. They can then be used to predict.\n    \"\"\"\n        \n    grads = {}\n    costs = []                            # to keep track of the cost\n    m = X.shape[1]                        # number of examples\n    layers_dims = [X.shape[0], 20, 3, 1]\n    \n    # Initialize parameters dictionary.\n    parameters = initialize_parameters(layers_dims)\n\n    # Loop (gradient descent)\n\n    for i in range(0, num_iterations):\n\n        # Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.\n        if keep_prob == 1:\n            a3, cache = forward_propagation(X, parameters)\n        elif keep_prob &lt; 1:\n            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n        \n        # Cost function\n        if lambd == 0:\n            cost = compute_cost(a3, Y)\n        else:\n            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)\n            \n        # Backward propagation.\n        assert (lambd == 0 or keep_prob == 1) \n\n        if lambd == 0 and keep_prob == 1:\n            grads = backward_propagation(X, Y, cache)\n        elif lambd != 0:\n            grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n        elif keep_prob &lt; 1:\n            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n        \n        # Update parameters.\n        parameters = update_parameters(parameters, grads, learning_rate)\n        \n        # Print the loss every 10000 iterations\n        if print_cost and i % 10000 == 0:\n            print(\"Cost after iteration {}: {}\".format(i, cost))\n        if print_cost and i % 1000 == 0:\n            costs.append(cost)\n    \n    # plot the cost\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('iterations (x1,000)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    \n    return parameters\n\n\nRegularization Techniques:\n\ncompute_cost_with_regularization(A3, Y, parameters, lambd): Computes the cost function for the model with L2 regularization to prevent overfitting.\n\n\n\ndef compute_cost_with_regularization(A3, Y, parameters, lambd):\n    \"\"\"\n    Implement the cost function with L2 regularization.\n    \n    Arguments:\n    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)\n    Y -- \"true\" labels vector, of shape (output size, number of examples)\n    parameters -- python dictionary containing parameters of the model\n    \n    Returns:\n    cost - value of the regularized loss function (formula (2))\n    \"\"\"\n    m = Y.shape[1]\n    W = {key: value for key, value in parameters.items() if key.startswith('W')}\n    \n    # Compute cross-entropy cost\n    cross_entropy_cost = compute_cost(A3, Y)\n    \n    # Compute L2 regularization term\n    L2_regularization_cost = (lambd / (2 * m)) * np.sum([np.sum(np.square(W[key])) for key in W])\n    \n    # Compute total regularized cost\n    cost = cross_entropy_cost + L2_regularization_cost\n    \n    return cost\n\n\nbackward_propagation_with_regularization(X, Y, cache, lambd): Implements backward propagation considering L2 regularization.\n\n\ndef backward_propagation_with_regularization(X, Y, cache, lambd):\n    \"\"\"\n    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n    \n    Arguments:\n    X -- input dataset, of shape (input size, number of examples)\n    Y -- \"true\" labels vector, of shape (output size, number of examples)\n    cache -- cache output from forward_propagation()\n    lambd -- regularization hyperparameter, scalar\n    \n    Returns:\n    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n    \"\"\"\n    \n    m = X.shape[1]\n    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n\n    dZ3 = A3 - Y\n    dW3 = (1 / m) * np.dot(dZ3, A2.T) + (lambd / m) * W3\n    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n\n    dA2 = np.dot(W3.T, dZ3)\n    dZ2 = np.multiply(dA2, np.int64(A2 &gt; 0))\n    dW2 = (1 / m) * np.dot(dZ2, A1.T) + (lambd / m) * W2\n    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n\n    dA1 = np.dot(W2.T, dZ2)\n    dZ1 = np.multiply(dA1, np.int64(A1 &gt; 0))\n    dW1 = (1 / m) * np.dot(dZ1, X.T) + (lambd / m) * W1\n    db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True)\n\n    gradients = {\n        \"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3, \"dA2\": dA2,\n        \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1,\n        \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1\n    }\n\n    return gradients\n\n\nDropout Technique:\n\nforward_propagation_with_dropout(X, parameters, keep_prob): Implements forward propagation with dropout, randomly deactivating neurons to enhance generalization.\n\n\n\ndef forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):\n    \"\"\"\n    Implements the forward propagation: LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; SIGMOID.\n    \n    Arguments:\n    X -- input dataset, of shape (2, number of examples)\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n                    W1 -- weight matrix of shape (20, 2)\n                    b1 -- bias vector of shape (20, 1)\n                    W2 -- weight matrix of shape (3, 20)\n                    b2 -- bias vector of shape (3, 1)\n                    W3 -- weight matrix of shape (1, 3)\n                    b3 -- bias vector of shape (1, 1)\n    keep_prob - probability of keeping a neuron active during drop-out, scalar\n    \n    Returns:\n    A3 -- last activation value, output of the forward propagation, of shape (1,1)\n    cache -- tuple, information stored for computing the backward propagation\n    \"\"\"\n    \n    np.random.seed(1)\n    \n    # Retrieve parameters\n    W1, b1, W2, b2, W3, b3 = parameters[\"W1\"], parameters[\"b1\"], parameters[\"W2\"], parameters[\"b2\"], parameters[\"W3\"], parameters[\"b3\"]\n    \n    # LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID\n    Z1 = np.dot(W1, X) + b1\n    A1 = relu(Z1)\n\n    D1 = np.random.rand(A1.shape[0], A1.shape[1])\n    D1 = (D1 &lt; keep_prob).astype(int)\n    A1 *= D1\n    A1 /= keep_prob\n    \n    Z2 = np.dot(W2, A1) + b2\n    A2 = relu(Z2)\n\n    D2 = np.random.rand(A2.shape[0], A2.shape[1])\n    D2 = (D2 &lt; keep_prob).astype(int)\n    A2 *= D2\n    A2 /= keep_prob\n    \n    Z3 = np.dot(W3, A2) + b3\n    A3 = sigmoid(Z3)\n    \n    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n    \n    return A3, cache\n\n\nbackward_propagation_with_dropout(X, Y, cache, keep_prob): Implements backward propagation considering dropout, managing deactivated neurons’ impact on gradients.\n\n\ndef backward_propagation_with_dropout(X, Y, cache, keep_prob):\n    \"\"\"\n    Implements the backward propagation of our baseline model to which we added dropout.\n    \n    Arguments:\n    X -- input dataset, of shape (2, number of examples)\n    Y -- \"true\" labels vector, of shape (output size, number of examples)\n    cache -- cache output from forward_propagation_with_dropout()\n    keep_prob - probability of keeping a neuron active during drop-out, scalar\n    \n    Returns:\n    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n    \"\"\"\n    \n    m = X.shape[1]\n    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n    \n    dZ3 = A3 - Y\n    dW3 = 1./m * np.dot(dZ3, A2.T)\n    db3 = 1./m * np.sum(dZ3, axis=1, keepdims=True)\n    dA2 = np.dot(W3.T, dZ3)\n    \n    # Apply dropout backward\n    dA2 *= D2\n    dA2 /= keep_prob\n    \n    dZ2 = np.multiply(dA2, np.int64(A2 &gt; 0))\n    dW2 = 1./m * np.dot(dZ2, A1.T)\n    db2 = 1./m * np.sum(dZ2, axis=1, keepdims=True)\n    \n    dA1 = np.dot(W2.T, dZ2)\n    \n    # Apply dropout backward\n    dA1 *= D1\n    dA1 /= keep_prob\n    \n    dZ1 = np.multiply(dA1, np.int64(A1 &gt; 0))\n    dW1 = 1./m * np.dot(dZ1, X.T)\n    db1 = 1./m * np.sum(dZ1, axis=1, keepdims=True)\n    \n    gradients = {\n        \"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3, \"dA2\": dA2,\n        \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1,\n        \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1\n    }\n    \n    return gradients\n\nThe model conducts forward propagation using rectified linear units (ReLU) and outputting with a sigmoid activation for binary classification tasks. It enables the incorporation of L2 regularization and dropout techniques as regularization strategies for mitigating overfitting.\nThe code provides a flexible neural network framework allowing the integration of L2 regularization and dropout to enhance model robustness and prevent overfitting, crucial for efficient and effective training in various scenarios."
  },
  {
    "objectID": "football.html#model-training-and-evaluation",
    "href": "football.html#model-training-and-evaluation",
    "title": "Exploring Regularization Techniques in Neural Networks for French Football Corporation",
    "section": "Model Training and Evaluation",
    "text": "Model Training and Evaluation\n\nModel Training and Evaluation (No Regularization)\nThis section encapsulates the training and evaluation process of the neural network model without any regularization technique applied. The code snippet showcases the training iterations and the associated cost function values. The training is performed on the provided training dataset (train_X, train_Y), and the trained model’s predictions are evaluated on both the training and test datasets (test_X, test_Y).\n\nparameters = model(train_X, train_Y)\nprint (\"On the training set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)\n\nCost after iteration 0: 0.6557412523481002\nCost after iteration 10000: 0.16329987525724216\nCost after iteration 20000: 0.13851642423254343\nOn the training set:\nAccuracy: 0.9478672985781991\nOn the test set:\nAccuracy: 0.915\n\n\n\n\n\nThe output displayed indicates the cost values at different iterations during the training process:\n\nInitial cost at iteration 0: 0.6557412523481002\nCost after 10000 iterations: 0.16329987525724216\nCost after 20000 iterations: 0.13851642423254343\n\nThis section demonstrates the iterative improvement of the model’s performance over multiple iterations, allowing insights into the convergence of the cost function during training. It provides a glimpse into the model’s learning process and its predictive capability on both the training and test sets without applying any form of regularization.\n\n\nModel Training and Evaluation (L2 Regularization)\nThis section involves training and evaluating the neural network model while applying L2 regularization with a regularization parameter (lambd) set to 0.7. The provided code snippet executes the model training on the train_X and train_Y datasets, followed by evaluating the trained model’s predictions on both the training and test datasets (test_X, test_Y).\n\nparameters = model(train_X, train_Y, lambd = 0.7)\nprint (\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)\n\nCost after iteration 0: 0.6974484493131264\nCost after iteration 10000: 0.2684918873282239\nCost after iteration 20000: 0.2680916337127301\nOn the train set:\nAccuracy: 0.9383886255924171\nOn the test set:\nAccuracy: 0.93\n\n\n\n\n\nThe displayed output showcases the cost values at different iterations during the training process:\n\nInitial cost at iteration 0: 0.6974484493131264\nCost after 10000 iterations: 0.2684918873282239\nCost after 20000 iterations: 0.2680916337127301\n\nThis section illustrates the impact of L2 regularization on the model’s learning process by monitoring the convergence of the cost function over multiple iterations. It provides insights into how the regularization parameter affects the model’s performance, controlling overfitting and enhancing generalization, as reflected in the cost function’s behavior on both the training and test datasets.\n\n\nModel Training and Evaluation (Dropout Regularization)\nThis section focuses on training and evaluating the neural network model with the application of Dropout regularization. The code snippet demonstrates the training of the model by specifying a dropout probability of 0.86 (keep_prob = 0.86) and a learning rate of 0.3 (learning_rate = 0.3).\n\nparameters = model(train_X, train_Y, keep_prob = 0.86, learning_rate = 0.3)\n\nprint (\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)\n\nCost after iteration 0: 0.6543912405149825\nCost after iteration 10000: 0.061016986574905605\nCost after iteration 20000: 0.060582435798513114\nOn the train set:\nAccuracy: 0.9289099526066351\nOn the test set:\nAccuracy: 0.95\n\n\n/tmp/ipykernel_39758/630415195.py:43: RuntimeWarning: divide by zero encountered in log\n  logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)\n/tmp/ipykernel_39758/630415195.py:43: RuntimeWarning: invalid value encountered in multiply\n  logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)\n\n\n\n\n\nThe output provided indicates the cost values at different iterations during the training process:\n\nInitial cost at iteration 0: 0.6543912405149825\nCost after 10000 iterations: 0.061016986574905605\nCost after 20000 iterations: 0.060582435798513114\n\nAdditionally, there might be a RuntimeWarning displayed due to encountering divide by zero or invalid value during the logarithmic calculations. The inclusion of Dropout regularization in the model training aims to mitigate overfitting by randomly dropping neurons during each iteration, enhancing the network’s generalization ability.\nThis section showcases the iterative reduction in the cost function, signifying the improvement in the model’s performance with the utilization of Dropout regularization, ultimately leading to better generalization on both the training and test datasets."
  },
  {
    "objectID": "football.html#visualization-of-results",
    "href": "football.html#visualization-of-results",
    "title": "Exploring Regularization Techniques in Neural Networks for French Football Corporation",
    "section": "Visualization of Results",
    "text": "Visualization of Results\nThe code snippet contains two functions, predict_dec and plot_decision_boundary, used for visualizing the decision boundary of a binary classification model.\n\npredict_dec(parameters, X): This function predicts the output using forward propagation based on the input data X and the model’s parameters. It returns a vector of predictions where red indicates 0 and blue indicates 1, with a classification threshold of 0.5.\nplot_decision_boundary(model, X, y): This function generates and plots the decision boundary for the binary classification model. It sets the minimum and maximum values for the x and y axes, creates a grid of points with a specified distance between them, predicts the function values for the entire grid, and plots the contour plot of the decision boundary. Additionally, it overlays the training examples on the plot, coloring them based on the class label (y).\n\nThe plot_decision_boundary function effectively illustrates how the model distinguishes between different classes by visualizing their decision boundary on a 2D plane. The plot showcases the regions where the model predicts each class, aiding in the interpretation and evaluation of the classifier’s performance.\n\ndef predict_dec(parameters, X):\n    \"\"\"\n    Used for plotting decision boundary.\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    X -- input data of size (m, K)\n    \n    Returns\n    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n    \"\"\"\n    \n    # Predict using forward propagation and a classification threshold of 0.5\n    a3, cache = forward_propagation(X, parameters)\n    predictions = (a3&gt;0.5)\n    return predictions\n\ndef plot_decision_boundary(model, X, y):\n    # Set min and max values and give it some padding\n    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n    h = 0.01\n    # Generate a grid of points with distance h between them\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    # Predict the function value for the whole grid\n    Z = model(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    # Plot the contour and training examples\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n    plt.ylabel('x2')\n    plt.xlabel('x1')\n    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)\n    plt.show()\n\n\nVisualizing Model Performance without Regularization\n\nplt.title(\"Model without regularization\")\naxes = plt.gca()\naxes.set_xlim([-0.75,0.40])\naxes.set_ylim([-0.75,0.65])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\n\n\n\n\n\nVisualizing Model Performance with L2 Regularization\n\nplt.title(\"Model with L2-regularization\")\naxes = plt.gca()\naxes.set_xlim([-0.75,0.40])\naxes.set_ylim([-0.75,0.65])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\n\n\n\n\n\nVisualizing Model Performance with Dropout\n\nplt.title(\"Model with dropout\")\naxes = plt.gca()\naxes.set_xlim([-0.75,0.40])\naxes.set_ylim([-0.75,0.65])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
  },
  {
    "objectID": "football.html#conclusion",
    "href": "football.html#conclusion",
    "title": "Exploring Regularization Techniques in Neural Networks for French Football Corporation",
    "section": "Conclusion:",
    "text": "Conclusion:\nThe analysis aimed to explore the effects of different regularization techniques, namely No Regularization, L2 Regularization, and Dropout, on the performance of a neural network model developed for the French Football Corporation’s problem.\n\nNo Regularization:\n\nAchieved an accuracy of approximately 94.79% on the training set and 91.50% on the test set.\nWithout any regularization, the model showed good performance on the training set but slightly lower performance on the unseen test set, indicating a potential issue of overfitting.\n\nL2 Regularization:\n\nDemonstrated an accuracy of around 93.84% on the training set and 93.00% on the test set.\nIntroducing L2 regularization slightly decreased the accuracy on the training set but significantly improved generalization to the test set. This technique helped alleviate overfitting and led to better performance on unseen data.\n\nDropout:\n\nAttained an accuracy of about 92.89% on the training set and an impressive 95.00% on the test set.\nEmploying Dropout regularization also reduced overfitting. It enhanced the model’s ability to generalize to new data, resulting in the highest accuracy among the tested regularization techniques on the test set.\n\n\n\nOverall Observations:\n\nThe experiment revealed the importance of regularization techniques in preventing overfitting and improving a model’s generalization to unseen data.\nL2 regularization and Dropout proved to be effective in mitigating overfitting, with Dropout exhibiting the most robust generalization performance on the test set in this context.\n\n\n\nRecommendations and Future Work:\n\nThe results emphasize the significance of incorporating regularization techniques, especially Dropout, when developing neural network models for tasks like predicting optimal positions in football.\nFurther investigation into different hyperparameter settings, architectures, or combinations of regularization techniques might yield even better model performance.\n\nThe findings from this analysis underscore the critical role that regularization techniques play in enhancing the robustness and generalization ability of neural network models for real-world applications."
  }
]